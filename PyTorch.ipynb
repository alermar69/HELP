{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "PyTorch.ipynb",
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alermar69/HELP/blob/master/PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIBq2GLIqtRG"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z7h_SuE-qyBW"
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tfs\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUXenYA1q16j"
   },
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCT6ioLMq8q6"
   },
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KqXiqTbJq4Vb"
   },
   "source": [
    "torch.HalfTensor      # 16 бит, floating point\n",
    "torch.FloatTensor     # 32 бита, floating point\n",
    "torch.DoubleTensor    # 64 бита, floating point\n",
    "\n",
    "torch.ShortTensor     # 16 бит, integer, signed\n",
    "torch.IntTensor       # 32 бита, integer, signed\n",
    "torch.LongTensor      # 64 бита, integer, signed\n",
    "\n",
    "torch.CharTensor      # 8 бит, integer, signed\n",
    "torch.ByteTensor      # 8 бит, integer, unsigned\n",
    "\n",
    "torch.FloatTensor([[1,2,3], [4,5,6]])\n",
    "torch.FloatTensor(2,3,4)\n",
    "torch.FloatTensor(3, 2, 4).zero_()\n",
    "\n",
    "a.type_as(torch.IntTensor())\n",
    "\n",
    "# np.reshape() == torch.view()\n",
    "b.view(3, 2)\n",
    "b.view(-1)\n",
    "\n",
    "a.sum(dim=0)\n",
    "a.sum(1)\n",
    "\n",
    "a.t()\n",
    "\n",
    "# вектор на вектор\n",
    "a.dot(b)\n",
    "a @ b\n",
    "\n",
    "# матрица на матрицу\n",
    "a.mm(b)\n",
    "a @ b\n",
    "\n",
    "# матрица на вектор\n",
    "a.mv(b)\n",
    "a @ b\n",
    "\n",
    "torch.from_numpy(a)\n",
    "x = a.numpy()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-tGXm-iu3rH"
   },
   "source": [
    "### Cuda"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vi5saqVQu6EZ"
   },
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available else torch.device('cpu')\n",
    "\n",
    "x.is_cuda\n",
    "device = torch.device(\"cuda:0\")\n",
    "x = x.to(device)\n",
    "c = a.cuda().mul(b.cuda()).cpu()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')          # CUDA-device object\n",
    "    y = torch.ones_like(x, device=device)  # create a tensor on GPU\n",
    "    x = x.to(device)                       # or just `.to(\"cuda\")`\n",
    "    z = x + y\n",
    "    print(z.to(\"cpu\", torch.double))\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgHs0TAkvdF3"
   },
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1c7KrEW5veSY"
   },
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 3, 3, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "y_pred = (x @ w1).clamp(min=0).mm(w2)\n",
    "\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "# calculate the gradients\n",
    "loss.backward()\n",
    "\n",
    "w1.grad"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMQz9o5ZwiVj"
   },
   "source": [
    "### Подготовка набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBwv3ocmv8UV"
   },
   "source": [
    "#### Compose"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "809-sKKAv_bW"
   },
   "source": [
    "import torchvision.transforms as tfs\n",
    "\n",
    "data_tfs = tfs.Compose([\n",
    "  tfs.ToTensor(),\n",
    "  tfs.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "root = './'\n",
    "train = MNIST(root, train=True,  transform=data_tfs, download=True)\n",
    "test  = MNIST(root, train=False, transform=data_tfs, download=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f3CdTGpwTbs"
   },
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rn59qH6swU6l"
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zIJvUt_COQ0Q"
   },
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wlOGw7rq7Tqu"
   },
   "source": [
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzGCg_0AxnIq"
   },
   "source": [
    "### Model and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAXlNKU2rvPY"
   },
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnw0FNgruNzT"
   },
   "source": [
    "код ниже подойдет для 90% задач"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vJNFUUpUruSq"
   },
   "source": [
    "for epoch in range(max_epochs):  # <--------------- итерируемся по датасету несколько раз\n",
    "    for k, dataloader in loaders.items():  # <----- несколько dataloader для train / valid / test\n",
    "        for x_batch, y_batch in dataloader:  # <--- итерируемся по датасету. Так как мы используем SGD а не GD, то берем батчи заданного размера\n",
    "            if k == \"train\":\n",
    "                model.train()  # <------------------ переводим модель в режим train\n",
    "                optimizer.zero_grad()  # <--------- обнуляем градиенты модели\n",
    "                outp = model(x_batch)\n",
    "                loss = criterion(outp, y_batch) # <-считаем \"лосс\" для логистической регрессии\n",
    "                loss.backward()  # <--------------- считаем градиенты\n",
    "                optimizer.step()  # <-------------- делаем шаг градиентного спуска\n",
    "            else:  # <----------------------------- test/eval\n",
    "                model.eval()  # <------------------ переводим модель в режим eval\n",
    "                with torch.no_grad():  # <--------- НЕ считаем градиенты\n",
    "                    outp = model(x_batch)  # <------------- получаем \"логиты\" из модели\n",
    "            count_metrics(outp, y_batch)  # <-------------- считаем метрики"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwjkDNoLWAFN"
   },
   "source": [
    "#### В ручную"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EgnUAVE9xoyL"
   },
   "source": [
    "features = 784\n",
    "classes = 10\n",
    "epochs = 3\n",
    "lr=1e-2\n",
    "history = []\n",
    "\n",
    "W = torch.FloatTensor(features, classes).uniform_(-1, 1) / features**0.5\n",
    "\n",
    "for i in range(epochs):\n",
    "  for x_batch, y_batch in train_loader:\n",
    "    x_batch = x_batch.reshape(x_batch.shape[0], -1)\n",
    "\n",
    "    logits = x_batch @ W\n",
    "    probabilities = torch.exp(logits) / torch.exp(logits).sum(dim=1, keepdims=True)\n",
    "    \n",
    "    loss = -torch.log(probabilities[range(batch_size), y_batch]).mean()\n",
    "    history.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    grad = W.grad\n",
    "    with torch.no_grad():\n",
    "      W -= lr * grad\n",
    "    W.grad.zero_()\n",
    "\n",
    "  print(f'{i+1},\\t loss: {history[-1]}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0mni6AQ1UIJ_"
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = 0\n",
    "batches = 0\n",
    "\n",
    "for x_batch, y_batch in test_loader:\n",
    "  batches += 1\n",
    "  x_batch = x_batch.view(x_batch.shape[0], -1)\n",
    "  y_batch = y_batch\n",
    "\n",
    "  preds = torch.argmax(x_batch @ W, dim=1)\n",
    "  acc += (preds==y_batch).cpu().numpy().mean()\n",
    "\n",
    "print(f'Test accuracy {acc / batches:.3}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD61Babfu3rp"
   },
   "source": [
    "Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d5T0Feonu9Zx"
   },
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(in_features, out_features, requires_grad=True))\n",
    "        self.bias = bias\n",
    "        if bias:\n",
    "            self.bias_term = nn.Parameter(torch.randn(1, out_features, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  x @ self.weights\n",
    "        if self.bias:\n",
    "            x +=  self.bias_term\n",
    "        return x\n",
    "\n",
    "X, y = make_moons(n_samples=10000, random_state=42, noise=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)\n",
    "\n",
    "X_train_t =  torch.from_numpy(X_train).type_as(torch.FloatTensor())\n",
    "y_train_t =  torch.from_numpy(y_train).type_as(torch.FloatTensor())\n",
    "X_val_t =  torch.from_numpy(X_val).type_as(torch.FloatTensor())\n",
    "y_val_t =  torch.from_numpy(y_val).type_as(torch.FloatTensor())\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "utils.set_global_seed(42)\n",
    "linear_regression = LinearRegression(2, 1)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(linear_regression.parameters(), lr=0.05)\n",
    "\n",
    "tol = 1e-3\n",
    "losses = []\n",
    "max_epochs = 100\n",
    "prev_weights = torch.zeros_like(linear_regression.weights)\n",
    "stop_it = False\n",
    "for epoch in range(max_epochs):\n",
    "    utils.set_global_seed(42 + epoch)\n",
    "    for it, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outp =  linear_regression(X_batch)\n",
    "        prob = F.sigmoid(outp)\n",
    "        loss =  -torch.log(torch.cat((prob[y_batch == 1], 1-prob[y_batch == 0]), 0)).mean()\n",
    "        loss.backward()\n",
    "        losses.append(loss.detach().flatten()[0])\n",
    "        optimizer.step()\n",
    "        probabilities =  prob\n",
    "        preds = (probabilities>0.5).type(torch.long)\n",
    "        batch_acc = (preds.flatten() == y_batch).type(torch.float32).sum()/y_batch.size(0)\n",
    "        if it % 500000 == 0:\n",
    "            print(f\"Iteration: {it + epoch*len(train_dataset)}\\nBatch accuracy: {batch_acc}\")\n",
    "        current_weights = linear_regression.weights.detach().clone()\n",
    "        if (prev_weights - current_weights).abs().max() < tol:\n",
    "            print(f\"\\nIteration: {it + epoch*len(train_dataset)}.Convergence. Stopping iterations.\")\n",
    "            stop_it = True\n",
    "            break\n",
    "        prev_weights = current_weights\n",
    "    if stop_it:\n",
    "        break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmeVrWr7WT6U"
   },
   "source": [
    "#### nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qEfS6aJNWkKt"
   },
   "source": [
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aPbym84yWriF"
   },
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Linear(features, 64),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(64, classes)\n",
    ")\n",
    "\n",
    "summary(model, (features,), batch_size=228)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # (logsoftmax + negative likelihood) in its core, applied to logits\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.99))\n",
    "\n",
    "epochs = 3\n",
    "history = []\n",
    "\n",
    "for i in range(epochs):\n",
    "  for x_batch, y_batch in train_loader:\n",
    "    x_batch = x_batch.view(x_batch.shape[0], -1).to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    logits = model(x_batch)\n",
    "\n",
    "    loss = criterion(logits, y_batch)\n",
    "    history.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f'{i+1},\\t loss: {history[-1]}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWOtL4ZcipV1"
   },
   "source": [
    "#### MyModule"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wgRzlKJtiuXq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(D_in, H), nn.Linear(H, D_out)])\n",
    "        self.my_useless_bias = nn.Parameter(torch.ones(1, H, requires_grad=True))\n",
    "        self.more_of_my_useless_biases = nn.ParameterList([\n",
    "            nn.Parameter(torch.ones(1, H, requires_grad=True)),\n",
    "            nn.Parameter(torch.ones(1, H, requires_grad=True)),\n",
    "            nn.Parameter(torch.ones(1, H, requires_grad=True))\n",
    "        ])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.linear_layers[0](X))\n",
    "        X += self.my_useless_bias\n",
    "        for b in self.more_of_my_useless_biases:\n",
    "            X += b\n",
    "        return F.softmax(self.linear_layers[1](X))\n",
    "    \n",
    "model = MyModule()\n",
    "list(model.parameters())\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}