{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "PyTorch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alermar69/HELP/blob/master/PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBq2GLIqtRG"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7h_SuE-qyBW"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as tfs\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUXenYA1q16j"
      },
      "source": [
        "## Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCT6ioLMq8q6"
      },
      "source": [
        "### Base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqXiqTbJq4Vb"
      },
      "source": [
        "torch.HalfTensor      # 16 бит, floating point\n",
        "torch.FloatTensor     # 32 бита, floating point\n",
        "torch.DoubleTensor    # 64 бита, floating point\n",
        "\n",
        "torch.ShortTensor     # 16 бит, integer, signed\n",
        "torch.IntTensor       # 32 бита, integer, signed\n",
        "torch.LongTensor      # 64 бита, integer, signed\n",
        "\n",
        "torch.CharTensor      # 8 бит, integer, signed\n",
        "torch.ByteTensor      # 8 бит, integer, unsigned\n",
        "\n",
        "torch.FloatTensor([[1,2,3], [4,5,6]])\n",
        "torch.FloatTensor(2,3,4)\n",
        "torch.FloatTensor(3, 2, 4).zero_()\n",
        "\n",
        "a.type_as(torch.IntTensor())\n",
        "\n",
        "# np.reshape() == torch.view()\n",
        "b.view(3, 2)\n",
        "b.view(-1)\n",
        "\n",
        "a.sum(dim=0)\n",
        "a.sum(1)\n",
        "\n",
        "a.t()\n",
        "\n",
        "# вектор на вектор\n",
        "a.dot(b)\n",
        "a @ b\n",
        "\n",
        "# матрица на матрицу\n",
        "a.mm(b)\n",
        "a @ b\n",
        "\n",
        "# матрица на вектор\n",
        "a.mv(b)\n",
        "a @ b\n",
        "\n",
        "torch.from_numpy(a)\n",
        "x = a.numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-tGXm-iu3rH"
      },
      "source": [
        "### Cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi5saqVQu6EZ"
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available else torch.device('cpu')\n",
        "\n",
        "x.is_cuda\n",
        "device = torch.device(\"cuda:0\")\n",
        "x = x.to(device)\n",
        "c = a.cuda().mul(b.cuda()).cpu()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')          # CUDA-device object\n",
        "    y = torch.ones_like(x, device=device)  # create a tensor on GPU\n",
        "    x = x.to(device)                       # or just `.to(\"cuda\")`\n",
        "    z = x + y\n",
        "    print(z.to(\"cpu\", torch.double))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgHs0TAkvdF3"
      },
      "source": [
        "### Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c7KrEW5veSY"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "N, D_in, H, D_out = 64, 3, 3, 10\n",
        "\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "y_pred = (x @ w1).clamp(min=0).mm(w2)\n",
        "\n",
        "loss = (y_pred - y).pow(2).sum()\n",
        "# calculate the gradients\n",
        "loss.backward()\n",
        "\n",
        "w1.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMQz9o5ZwiVj"
      },
      "source": [
        "### Подготовка набора данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBwv3ocmv8UV"
      },
      "source": [
        "#### Compose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "809-sKKAv_bW"
      },
      "source": [
        "import torchvision.transforms as tfs\n",
        "\n",
        "data_tfs = tfs.Compose([\n",
        "  tfs.ToTensor(),\n",
        "  tfs.Normalize((0.5), (0.5))\n",
        "])\n",
        "\n",
        "root = './'\n",
        "train = MNIST(root, train=True,  transform=data_tfs, download=True)\n",
        "test  = MNIST(root, train=False, transform=data_tfs, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f3CdTGpwTbs"
      },
      "source": [
        "#### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn59qH6swU6l"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "x_batch, y_batch = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIJvUt_COQ0Q"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlOGw7rq7Tqu"
      },
      "source": [
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzGCg_0AxnIq"
      },
      "source": [
        "### Model and Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAXlNKU2rvPY"
      },
      "source": [
        "#### Train loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnw0FNgruNzT"
      },
      "source": [
        "код ниже подойдет для 90% задач"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJNFUUpUruSq"
      },
      "source": [
        "for epoch in range(max_epochs):  # <--------------- итерируемся по датасету несколько раз\n",
        "    for k, dataloader in loaders.items():  # <----- несколько dataloader для train / valid / test\n",
        "        for x_batch, y_batch in dataloader:  # <--- итерируемся по датасету. Так как мы используем SGD а не GD, то берем батчи заданного размера\n",
        "            if k == \"train\":\n",
        "                model.train()  # <------------------ переводим модель в режим train\n",
        "                optimizer.zero_grad()  # <--------- обнуляем градиенты модели\n",
        "                outp = model(x_batch)\n",
        "                loss = criterion(outp, y_batch) # <-считаем \"лосс\" для логистической регрессии\n",
        "                loss.backward()  # <--------------- считаем градиенты\n",
        "                optimizer.step()  # <-------------- делаем шаг градиентного спуска\n",
        "            else:  # <----------------------------- test/eval\n",
        "                model.eval()  # <------------------ переводим модель в режим eval\n",
        "                with torch.no_grad():  # <--------- НЕ считаем градиенты\n",
        "                    outp = model(x_batch)  # <------------- получаем \"логиты\" из модели\n",
        "            count_metrics(outp, y_batch)  # <-------------- считаем метрики"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwjkDNoLWAFN"
      },
      "source": [
        "#### В ручную"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgnUAVE9xoyL"
      },
      "source": [
        "features = 784\n",
        "classes = 10\n",
        "epochs = 3\n",
        "lr=1e-2\n",
        "history = []\n",
        "\n",
        "W = torch.FloatTensor(features, classes).uniform_(-1, 1) / features**0.5\n",
        "\n",
        "for i in range(epochs):\n",
        "  for x_batch, y_batch in train_loader:\n",
        "    x_batch = x_batch.reshape(x_batch.shape[0], -1)\n",
        "\n",
        "    logits = x_batch @ W\n",
        "    probabilities = torch.exp(logits) / torch.exp(logits).sum(dim=1, keepdims=True)\n",
        "    \n",
        "    loss = -torch.log(probabilities[range(batch_size), y_batch]).mean()\n",
        "    history.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    grad = W.grad\n",
        "    with torch.no_grad():\n",
        "      W -= lr * grad\n",
        "    W.grad.zero_()\n",
        "\n",
        "  print(f'{i+1},\\t loss: {history[-1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mni6AQ1UIJ_"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acc = 0\n",
        "batches = 0\n",
        "\n",
        "for x_batch, y_batch in test_loader:\n",
        "  batches += 1\n",
        "  x_batch = x_batch.view(x_batch.shape[0], -1)\n",
        "  y_batch = y_batch\n",
        "\n",
        "  preds = torch.argmax(x_batch @ W, dim=1)\n",
        "  acc += (preds==y_batch).cpu().numpy().mean()\n",
        "\n",
        "print(f'Test accuracy {acc / batches:.3}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD61Babfu3rp"
      },
      "source": [
        "Логистическая регрессия"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5T0Feonu9Zx"
      },
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(in_features, out_features, requires_grad=True))\n",
        "        self.bias = bias\n",
        "        if bias:\n",
        "            self.bias_term = nn.Parameter(torch.randn(1, out_features, requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x =  x @ self.weights\n",
        "        if self.bias:\n",
        "            x +=  self.bias_term\n",
        "        return x\n",
        "\n",
        "X, y = make_moons(n_samples=10000, random_state=42, noise=0.1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)\n",
        "\n",
        "X_train_t =  torch.from_numpy(X_train).type_as(torch.FloatTensor())\n",
        "y_train_t =  torch.from_numpy(y_train).type_as(torch.FloatTensor())\n",
        "X_val_t =  torch.from_numpy(X_val).type_as(torch.FloatTensor())\n",
        "y_val_t =  torch.from_numpy(y_val).type_as(torch.FloatTensor())\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "utils.set_global_seed(42)\n",
        "linear_regression = LinearRegression(2, 1)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(linear_regression.parameters(), lr=0.05)\n",
        "\n",
        "tol = 1e-3\n",
        "losses = []\n",
        "max_epochs = 100\n",
        "prev_weights = torch.zeros_like(linear_regression.weights)\n",
        "stop_it = False\n",
        "for epoch in range(max_epochs):\n",
        "    utils.set_global_seed(42 + epoch)\n",
        "    for it, (X_batch, y_batch) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outp =  linear_regression(X_batch)\n",
        "        prob = F.sigmoid(outp)\n",
        "        loss =  -torch.log(torch.cat((prob[y_batch == 1], 1-prob[y_batch == 0]), 0)).mean()\n",
        "        loss.backward()\n",
        "        losses.append(loss.detach().flatten()[0])\n",
        "        optimizer.step()\n",
        "        probabilities =  prob\n",
        "        preds = (probabilities>0.5).type(torch.long)\n",
        "        batch_acc = (preds.flatten() == y_batch).type(torch.float32).sum()/y_batch.size(0)\n",
        "        if it % 500000 == 0:\n",
        "            print(f\"Iteration: {it + epoch*len(train_dataset)}\\nBatch accuracy: {batch_acc}\")\n",
        "        current_weights = linear_regression.weights.detach().clone()\n",
        "        if (prev_weights - current_weights).abs().max() < tol:\n",
        "            print(f\"\\nIteration: {it + epoch*len(train_dataset)}.Convergence. Stopping iterations.\")\n",
        "            stop_it = True\n",
        "            break\n",
        "        prev_weights = current_weights\n",
        "    if stop_it:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmeVrWr7WT6U"
      },
      "source": [
        "#### nn.Sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEfS6aJNWkKt"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPbym84yWriF"
      },
      "source": [
        "model = nn.Sequential(\n",
        "  nn.Linear(features, 64),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(64, classes)\n",
        ")\n",
        "\n",
        "summary(model, (features,), batch_size=228)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # (logsoftmax + negative likelihood) in its core, applied to logits\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.99))\n",
        "\n",
        "epochs = 3\n",
        "history = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  for x_batch, y_batch in train_loader:\n",
        "    x_batch = x_batch.view(x_batch.shape[0], -1).to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    logits = model(x_batch)\n",
        "\n",
        "    loss = criterion(logits, y_batch)\n",
        "    history.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'{i+1},\\t loss: {history[-1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWOtL4ZcipV1"
      },
      "source": [
        "#### MyModule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgRzlKJtiuXq"
      },
      "source": [
        "class MyModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_layers = nn.ModuleList([nn.Linear(D_in, H), nn.Linear(H, D_out)])\n",
        "        self.my_useless_bias = nn.Parameter(torch.ones(1, H, requires_grad=True))\n",
        "        self.more_of_my_useless_biases = nn.ParameterList([\n",
        "            nn.Parameter(torch.ones(1, H, requires_grad=True)),\n",
        "            nn.Parameter(torch.ones(1, H, requires_grad=True)),\n",
        "            nn.Parameter(torch.ones(1, H, requires_grad=True))\n",
        "        ])\n",
        "        \n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.linear_layers[0](X))\n",
        "        X += self.my_useless_bias\n",
        "        for b in self.more_of_my_useless_biases:\n",
        "            X += b\n",
        "        return F.softmax(self.linear_layers[1](X))\n",
        "    \n",
        "model = MyModule()\n",
        "list(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjPQP7aofHh-"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyPx52Cntvs8"
      },
      "source": [
        "### MNIST (модель LeNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBcnFpGNt3ZU"
      },
      "source": [
        "import os\n",
        "from catalyst.contrib.datasets import MNIST\n",
        "\n",
        "train_dataset = MNIST(root=os.getcwd(), train=True, download=True)\n",
        "val_dataset = MNIST(root=os.getcwd(), train=False)\n",
        "train_dataloader =  DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "valid_dataloader =  DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = LeNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "loaders = {\"train\": train_dataloader, \"valid\": valid_dataloader}\n",
        "\n",
        "max_epochs = 10\n",
        "accuracy = {\"train\": [], \"valid\": []}\n",
        "for epoch in range(max_epochs):\n",
        "    epoch_correct = 0\n",
        "    epoch_all = 0\n",
        "    utils.set_global_seed(42+epoch)\n",
        "    for k, dataloader in loaders.items():\n",
        "        for x_batch, y_batch in dataloader:\n",
        "            if k == \"train\":\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                outp = model(x_batch.type(torch.float32).unsqueeze(1))\n",
        "            else:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    outp = model(x_batch.type(torch.float32).unsqueeze(1))\n",
        "            preds = outp.argmax(-1)\n",
        "            correct = (y_batch == preds).sum()\n",
        "            all =  x_batch.shape[0]\n",
        "            epoch_correct += correct.item()\n",
        "            epoch_all += all\n",
        "            if k == \"train\":\n",
        "                loss = criterion(outp, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        if k == \"train\":\n",
        "            print(f\"Epoch: {epoch+1}\")\n",
        "        print(f\"Loader: {k}. Accuracy: {epoch_correct/epoch_all}\")\n",
        "        accuracy[k].append(epoch_correct/epoch_all)\n",
        "\n",
        "lenet_accuracy = accuracy[\"valid\"]\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.title(\"Valid accuracy\")\n",
        "# plt.plot(range(max_epochs), relu_accuracy, label=\"ReLU activation\", linewidth=2)\n",
        "# plt.plot(range(max_epochs), leaky_relu_accuracy, label=\" activation\", linewidth=2)\n",
        "# plt.plot(range(max_epochs), elu_accuracy, label=\"ELU activation\", linewidth=2)\n",
        "plt.plot(range(max_epochs), lenet_accuracy, label=\"LeNet\", linewidth=2)\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTfgBix5fW4T"
      },
      "source": [
        "### CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t0nqLHSfYK6"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk-bAxGxfqMS"
      },
      "source": [
        "class MyConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # вызов конструктора предка\n",
        "        super().__init__()\n",
        "        # необходмо заранее знать, сколько каналов у картинки (сейчас = 1),\n",
        "        # которую будем подавать в сеть, больше ничего\n",
        "        # про входящие картинки знать не нужно\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(6 * 6 * 128, 128)  # !!!\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #(32,32)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        #(14,14)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        #(6,6)\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1, 6 * 6 * 128)  # !!!\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0qLSQnSf0gD"
      },
      "source": [
        "# пример взят из официального туториала: \n",
        "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "net = MyConvNet().to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# итерируемся\n",
        "for epoch in tqdm_notebook(range(10)):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, batch in enumerate(tqdm_notebook(trainloader)):\n",
        "        # так получаем текущий батч\n",
        "        X_batch, y_batch = batch\n",
        "\n",
        "        # обнуляем веса\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        y_pred = net(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # выведем текущий loss\n",
        "        running_loss += loss.item()\n",
        "        # выводем качество каждые 2000 батчей\n",
        "        if i % 2000 == 1999:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Обучение закончено')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMfdFkEnf6O6"
      },
      "source": [
        "# Посмотрим на accuracy на тестовом датасете:\n",
        "\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        y_pred = net(images.to(device))#.view(4, -1))\n",
        "        _, predicted = torch.max(y_pred, 1)\n",
        "        c = (predicted.cpu().detach() == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}